"""
AI Manager - Gemini AI ê´€ë¦¬
"""

import os
from typing import List, Optional, AsyncGenerator, Dict
import asyncio

# Google Gemini
try:
    from google import genai
    from google.genai import types
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


class AIManager:
    """Gemini AI ê´€ë¦¬ì"""

    def __init__(self):
        # API í‚¤ ë¡œë“œ
        self.gemini_key = os.getenv("GEMINI_API_KEY")

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.gemini_client = None

        if GEMINI_AVAILABLE and self.gemini_key:
            self.gemini_client = genai.Client(api_key=self.gemini_key)
            print("âœ… Google (Gemini) ì—°ê²° ì™„ë£Œ")
        else:
            raise RuntimeError("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GEMINI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    def get_available_ais(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª©ë¡"""
        return ["Gemini"] if self.gemini_client else []
    
    def format_context(self, context: Optional[str], files: Optional[List[Dict]] = None) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        parts = []
        
        if context:
            parts.append(f"<ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´>\n{context}\n</ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´>")
        
        if files:
            file_list = "\n".join([f"- {f['display_name']}" for f in files])
            parts.append(f"<ì°¸ê³  íŒŒì¼ ëª©ë¡>\n{file_list}\n</ì°¸ê³  íŒŒì¼ ëª©ë¡>")
        
        if parts:
            return "\n\n" + "\n\n".join(parts) + "\n\nìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        return ""
    
    def format_history(self, history: List[dict], limit: int = 5) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        if not history:
            return ""
        
        recent_history = history[-limit*3:]  # ìµœê·¼ Nê°œ ëŒ€í™”
        formatted = []
        
        for msg in recent_history:
            if msg["type"] == "user":
                formatted.append(f"User: {msg['message']}")
            elif msg["type"] == "ai":
                formatted.append(f"{msg['ai_name']}: {msg['message']}")
        
        if formatted:
            return "\n\n<ì´ì „ ëŒ€í™”>\n" + "\n".join(formatted) + "\n</ì´ì „ ëŒ€í™”>\n"
        return ""
    
    async def get_response(
        self,
        ai_name: str,
        message: str,
        context: Optional[str] = None,
        history: Optional[List[dict]] = None,
        file_search_context: Optional[dict] = None,
        character_system_prompt: Optional[str] = None
    ) -> str:
        """AI ì‘ë‹µ ìƒì„±"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_message = message

        # ìºë¦­í„° ì±„íŒ…ì´ ì•„ë‹ ë•Œë§Œ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
        # (ìºë¦­í„° ì±„íŒ…ì€ character_system_promptì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ)
        if not character_system_prompt and file_search_context and file_search_context.get("searched_context"):
            rag_context = file_search_context["searched_context"]
            full_message = f"""<ì°¸ê³  ë¬¸ì„œ ë‚´ìš©>
{rag_context}
</ì°¸ê³  ë¬¸ì„œ ë‚´ìš©>

ì‚¬ìš©ì ì§ˆë¬¸: {message}

**ì¤‘ìš” ì§€ì¹¨:**
- ìœ„ ë¬¸ì„œ ë‚´ìš©ì€ ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ë•Œë§Œ í™œìš©í•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ ì¼ë°˜ì ì¸ ë‚´ìš©(ì¸ì‚¬, ë‚ ì”¨, ì¼ìƒ ëŒ€í™” ë“±)ì´ë¼ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ë¬´ì‹œí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ "ë¬¸ì„œì—ì„œ", "íŒŒì¼ì—ì„œ", "ì—…ë¡œë“œí•œ ìë£Œì—ì„œ" ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ë¬¸ì„œ ë‚´ìš©ê³¼ ëª…í™•íˆ ê´€ë ¨ëœ ì§ˆë¬¸ì¼ ë•Œë§Œ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- ë¬¸ì„œë¥¼ ì°¸ì¡°í•  ë•ŒëŠ” ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."""

        if context:
            full_message += self.format_context(context)
        if history:
            full_message = self.format_history(history) + full_message

        if ai_name == "Gemini":
            return await self._get_gemini_response(full_message, file_search_context, character_system_prompt)
        else:
            raise ValueError(f"Geminië§Œ ì§€ì›ë©ë‹ˆë‹¤. ìš”ì²­ëœ AI: {ai_name}")
    
    async def get_response_stream(
        self,
        ai_name: str,
        message: str,
        context: Optional[str] = None,
        history: Optional[List[dict]] = None,
        file_search_context: Optional[dict] = None,
        character_system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """AI ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_message = message

        # ìºë¦­í„° ì±„íŒ…ì´ ì•„ë‹ ë•Œë§Œ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
        # (ìºë¦­í„° ì±„íŒ…ì€ character_system_promptì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ)
        if not character_system_prompt and file_search_context and file_search_context.get("searched_context"):
            rag_context = file_search_context["searched_context"]
            full_message = f"""<ì°¸ê³  ë¬¸ì„œ ë‚´ìš©>
{rag_context}
</ì°¸ê³  ë¬¸ì„œ ë‚´ìš©>

ì‚¬ìš©ì ì§ˆë¬¸: {message}

**ì¤‘ìš” ì§€ì¹¨:**
- ìœ„ ë¬¸ì„œ ë‚´ìš©ì€ ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ë•Œë§Œ í™œìš©í•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ ì¼ë°˜ì ì¸ ë‚´ìš©(ì¸ì‚¬, ë‚ ì”¨, ì¼ìƒ ëŒ€í™” ë“±)ì´ë¼ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ë¬´ì‹œí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ "ë¬¸ì„œì—ì„œ", "íŒŒì¼ì—ì„œ", "ì—…ë¡œë“œí•œ ìë£Œì—ì„œ" ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ë¬¸ì„œ ë‚´ìš©ê³¼ ëª…í™•íˆ ê´€ë ¨ëœ ì§ˆë¬¸ì¼ ë•Œë§Œ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- ë¬¸ì„œë¥¼ ì°¸ì¡°í•  ë•ŒëŠ” ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."""

        if context:
            full_message += self.format_context(context)
        if history:
            full_message = self.format_history(history) + full_message

        if ai_name == "Gemini":
            async for chunk in self._get_gemini_response_stream(full_message, file_search_context, character_system_prompt):
                yield chunk
        else:
            yield f"Geminië§Œ ì§€ì›ë©ë‹ˆë‹¤. ìš”ì²­ëœ AI: {ai_name}"

    # ==================== Gemini ====================

    async def _get_gemini_response(self, message: str, file_search_context: Optional[dict] = None, character_system_prompt: Optional[str] = None) -> str:
        """Gemini ì‘ë‹µ (ì¼ë°˜) - File Search Store ì§€ì›"""
        if not self.gemini_client:
            return "Geminië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                loop = asyncio.get_event_loop()

                # ìºë¦­í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’)
                system_instruction = character_system_prompt if character_system_prompt else "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."

                # File Search Store í™œìš© ì—¬ë¶€ íŒë‹¨
                if file_search_context and file_search_context.get("store_name"):
                    store_name = file_search_context["store_name"]
                    print(f"ğŸ” File Search Store ì‚¬ìš©: {store_name}")

                    # File Search Tool ì„¤ì •
                    response = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content(
                            model="gemini-2.5-flash",
                            contents=message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000,
                                system_instruction=system_instruction,
                                tools=[
                                    types.Tool(
                                        file_search=types.FileSearch(
                                            file_search_store_names=[store_name]
                                        )
                                    )
                                ]
                            )
                        )
                    )
                else:
                    # File Search ë¯¸ì‚¬ìš© (ì¼ë°˜ ëª¨ë“œ)
                    response = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content(
                            model="gemini-2.5-flash",
                            contents=message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000,
                                system_instruction=system_instruction
                            )
                        )
                    )

                return response.text
            except Exception as e:
                error_msg = str(e)
                # Rate limit, quota, ì„œë²„ ì˜¤ë¥˜ ë“±ì— ëŒ€í•´ ì¬ì‹œë„
                if any(keyword in error_msg.lower() for keyword in ["rate_limit", "quota", "timeout", "503", "502", "500", "429", "resource_exhausted"]):
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Gemini API ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                return f"Gemini ì˜¤ë¥˜: {error_msg}"

        return "Geminiê°€ í˜„ì¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    async def _get_gemini_response_stream(self, message: str, file_search_context: Optional[dict] = None, character_system_prompt: Optional[str] = None) -> AsyncGenerator[str, None]:
        """Gemini ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°) - File Search Store ì§€ì›"""
        if not self.gemini_client:
            yield "Geminië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                loop = asyncio.get_event_loop()

                # ìºë¦­í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’)
                system_instruction = character_system_prompt if character_system_prompt else "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."

                # File Search Store í™œìš© ì—¬ë¶€ íŒë‹¨
                if file_search_context and file_search_context.get("store_name"):
                    store_name = file_search_context["store_name"]
                    print(f"ğŸ” File Search Store ì‚¬ìš© (ìŠ¤íŠ¸ë¦¬ë°): {store_name}")

                    # File Search Tool ì„¤ì •
                    stream = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content_stream(
                            model="gemini-2.5-flash",
                            contents=message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000,
                                system_instruction=system_instruction,
                                tools=[
                                    types.Tool(
                                        file_search=types.FileSearch(
                                            file_search_store_names=[store_name]
                                        )
                                    )
                                ]
                            )
                        )
                    )
                else:
                    # File Search ë¯¸ì‚¬ìš© (ì¼ë°˜ ëª¨ë“œ)
                    stream = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content_stream(
                            model="gemini-2.5-flash",
                            contents=message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000,
                                system_instruction=system_instruction
                            )
                        )
                    )

                for chunk in stream:
                    if chunk.text:
                        yield chunk.text
                        await asyncio.sleep(0.01)
                return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
            except Exception as e:
                error_msg = str(e)
                # Rate limit, quota, ì„œë²„ ì˜¤ë¥˜ ë“±ì— ëŒ€í•´ ì¬ì‹œë„
                if any(keyword in error_msg.lower() for keyword in ["rate_limit", "quota", "timeout", "503", "502", "500", "429", "resource_exhausted"]):
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Gemini API ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                yield f"Gemini ì˜¤ë¥˜: {error_msg}"
                return

        yield "Geminiê°€ í˜„ì¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
